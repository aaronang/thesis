\documentclass[twoside,a4paper,11pt]{memoir}
\input{preamble}

\title{DDU}
\subtitle{Master's Thesis}

\author{Aaron Ang}
\authoremail{\url{a.w.z.ang@student.tudelft.nl}}
\birthplace{Amstelveen, The Netherlands}
\studentid{4139194}

\company{
\includegraphics[height=1.3cm]{figures/parc_logo.png}\\
PARC, a Xerox company\\
3333 Coyote Hill Road\\
Palo Alto, CA 94304\\
\url{www.parc.com}
}

\chair{Prof. Dr. A. van Deursen, Faculty EEMCS, TU Delft}
\supervisor{Prof. Dr. A. van Deursen, Faculty EEMCS, TU Delft}
\externalsupervisor{Prof. Dr. R. Maranhao, University of Lisbon}

\setcounter{tocdepth}{1}
\setsecnumdepth{subsection}
\maxsecnumdepth{subsection}

\begin{document}

\frontmatter
\thispagestyle{empty}
\maketitle
\makeformaltitlepages{ABSTRACT}

\chapter{Preface}
\label{ch:preface}

This is where you thank people for helping you etc.

\vskip1cm
\begin{flushright}
\theauthor\\
Delft, The Netherlands \\
\today\\
\end{flushright}

\cleardoublepage\tableofcontents
\cleardoublepage\listoffigures
\cleardoublepage\mainmatter

\chapter{Introduction}
\label{ch:introduction}
Software systems are complex and error-prone, likely to expose failures to the end user.
When a failure occurs, the developer has to debug the system to eliminate the failure.
This debugging process can be described in three phases \cite{parnin2011automated}.
In the first phase, the developer has to pinpoint the fault, also known as the root cause, in code that causes the failure.
In the second phase, the developer has to develop an understanding of the root cause and its context.
Finally, in the third phase, the developer has to implement a patch that corrects the behavior of the system.
This process is time-consuming and can account for 30\% to 90\% of the software development cycle \cite{robbins2003debugging, beizer2003software, britton2013reversible}.

Traditionally, developers use four different approaches to debug a software system, namely program logging, assertions, breakpoints and profiling \cite{wong2016survey}.
Program logging is the act of inserting \emph{print} statements in the code to observe program state information during execution.
Assertions are constraints that can be added to a program that have to evaluate to true during execution time.
Breakpoints allow the developer to pause the software system during execution, and observe and modify variable values.
Profiling is used to perform runtime analysis and collect metrics on, for example, execution speed and memory usage.
These techniques provide an intuitive approach to localize the root cause of a failure, but, as one might expect, are less effective in the massive size and scale of software systems today.

Therefore, in the last decades a lot of research has been performed on improving and developing \emph{advanced} fault localization techniques \cite{wong2016survey} such that they are applicable to the software systems of today.
Specifically, a prominent fault localization technique is spectrum-based fault localization (SBFL).
SBFL techniques pinpoint faults in code based on execution information of a program, also known as a program spectrum \cite{reps1997use}.
It does this by outputting a list of suspicious components, for example statements or methods, ranked by their suspiciousness.
Intuitively, if a statement is executed primarily during failed executions, then this statement might be assigned a higher suspiciousness score.
Similarly, if a statement is executed primarily during successful executions, then this statement might be assigned a lower suspiciousness score.

While SBFL techniques are promising for debugging purposes, these techniques are dependent on the quality of a test suite.
Currently, test suites are optimized with respect to adequacy measurements that focus on error detection, e.g. branch coverage, line coverage.
However, Perez \etal~\cite{DBLP:conf/icse/PerezAD17} show evidence that optimizing a test suite with respect to DDU --- a metric to quantify the test suite's diagnosability --- improves the diagnostic performance of SBFL by 34\% compared to a test suite optimized with respect to branch coverage.
The goal of DDU is to capture diagnosability and to serve as a complementary metric to code coverage for developers to use to improve the test suite's diagnosability.

\section{Problem Definition}
\label{sec:problem_definition}
Currently, when the DDU is computed for a given test suite, its value is in the domain $[ 0, 1 ]$, where $0$ suggests that the test suite's diagnosability is low, and $1$ suggests that the test suite's diagnosability is high.
The problem with this value is that the developer does not know how to extend or update the test suite given a DDU value.
For example, when the test suite's DDU is equal to $0.1$, the developer does not know how to write tests that improve the DDU.
In other words, time spent on software debugging cannot be reduced using DDU because its practical implications are unclear to the developer.

\section{Goal}
Although DDU is currently not usable in practice, Perez \etal \cite{DBLP:conf/icse/PerezAD17} have shown that optimizing a test suite with respect to DDU can yield a 34\% gain in diagnostic performance using SBFL.
In addition, having a test suite with a high diagnosability could possibly reduce the time spent debugging because the fault is easier to find manually.
Therefore, the goal of this thesis is to find ways to make DDU usable in practice.
In other words, we explore possibilities to convey DDU to the developer such that the developer knows what kind of tests to write to improve the system's diagnosability.
To be able to


\section{Structure of Report}
The structure of this report is as follows.
\todo{Update once all chapters are done.}

\chapter{Background}
\label{ch:background}

In this chapter, we discuss topics that are relevant to understanding the following chapters.
First, we discuss spectrum-based reasoning, which is used in the experiments to compute the diagnostic performance.
Second, we discuss the metric used to evaluate the diagnostic performance of spectrum-based fault localization (SBFL) techniques.
Then, we explain the definition of diagnosability and diagnosability assessment metrics.

\section{Spectrum-Based Reasoning (SBR)}
Spectrum-based reasoning is a spectrum-based fault localization technique that leverages a Bayesian reasoning framework to diagnose fault candidates that could potentially be the root cause for a given software failure \cite{abreu2009spectrum}.
In SBR, we define a finite set $\mathcal{C} = \langle c_1, c_2, \ldots, c_M \rangle$ of $M$ system components, and the finite set $\mathcal{T} = \langle t_1, t_2, \ldots, t_N \rangle$ of $N$ system transactions, such as test executions.
The outcomes of all system transactions are defined as an error vector $e = \langle e_1, e_2, \ldots, e_N \rangle$, where $e_i = 1$ indicates that transaction $t_i$ has failed and $e_i = 0$ otherwise.
To keep track of which system components were executed during which system transactions, we construct a $N \times M$ activity matrix $\mathcal{A}$, where $\mathcal{A}_{ij} = 1$ indicates that component $c_j$ was hit during transaction $t_i$.
The pair $(\mathcal{A}, e)$ is also known as a program spectrum, which was first coined by Reps \etal \cite{reps1997use}.

SBR distinguishes itself from SBFL techniques by leveraging a reasoning framework.
More specifically, the diagnostic report is generated by reasoning about the program spectrum instead of using a so-called similarity coefficient.
The two main phases of SBR are candidate generation and candidate ranking:
\begin{enumerate}
  \item In the candidate generation phase, a set $\mathcal{D} = \langle d_1, d_2, \dots, d_k \rangle$ is constructed using a minimal hitting set (MHS) algorithm to cover all failing transactions, where each candidate $d_i$ is a subset of $\mathcal{C}$.
  An MHS algorithm is used to prevent generation of of a possibly exponential number of diagnostic candidates \cite{Abreu:2009:SMF:1747491.1747511}.

  \item In the candidate ranking phase, the fault probability for each candidate $d_i$ is computed using the Naive Bayes rule \cite{Abreu:2009:SMF:1747491.1747511}:
  \begin{equation}
    P(d_i | (\mathcal{A}, e)) = P(d_i) \cdot \prod_{j \in 1..N} \frac{P((\mathcal{A}_j, e_j) | d_i)}{P(\mathcal{A}_j)}
  \end{equation}
  $P(d_i)$ is the prior probability, i.e. the probability that $d_i$ is faulty without any evidence.
  $P(\mathcal{A}_j)$ is a normalizing term that is identical for all candidates.
  $P((\mathcal{A}_j, e_j) | d_i)$ changes the prior probability with every new observation from the program spectrum.
  This term can be computed using maximum likelihood estimation.
\end{enumerate}

The final output of SBR is a diagnostic report, which is a list of components, e.g. statements or methods, ranked by their suspiciousness.
In the experiments of this thesis, we will make use of \emph{Barinel} \cite{Abreu:2009:SMF:1747491.1747511}, which implements the spectrum-based reasoning technique to perform software fault localization.

\section{Evaluation of Diagnosis}
Presently, cost of diagnosis $C_d$ and wasted effort \cite{Abreu:2009:SMF:1747491.1747511, 7390282, 6693085, Steimann:2013:TVV:2483760.2483767, DBLP:conf/icse/PerezAD17} are the most prevalent evaluation metrics for SFL techniques.
In essence, $C_d$ computes the number of components that have to be inspected before the actual fault is investigated in the diagnostic report.
When $C_d = 0$, it indicates that the actual fault is ranked first in the diagnostic report and, therefore, no effort is wasted investigating diagnosed components that are non-faulty.

Wasted effort (or effort) is the cost of diagnosis normalized by the number of diagnosed components in the diagnostic report.
Both evaluation metrics assume \emph{perfect bug understanding}, which has been pointed out by Parnin and Orso \cite{Parnin:2011:ADT:2001420.2001445} as a non-realistic assumption.
However, cost of diagnosis and effort serve as an objective evaluation metric that can be used for comparison and therefore will also be used in this study.

\section{Diagnosability}
Diagnosability is the property of faults to be easily and precisely located \cite{730889}.
In other words, given that a fault exists in a software system, if the test suite's diagnosability is high and we would perform SFL using an automated debugging technique, then the faulty component would be ranked high in the diagnostic report, i.e. the wasted effort would be low.
On the contrary, if the test suite's diagnosability is low and we would perform SFL using an automated debugging technique, then the faulty component would be ranked low in the diagnostic report, i.e. the wasted effort would be high.

\subsection{Diagnosability Metric: Entropy}
The optimal diagnosability is achieved by having an exhaustive test suite that would exercise any combination of software components.
This way, any fault, whether it involves a single component or multiple components, can be diagnosed using an automated debugging technique with 100\% accuracy.
Perez \etal \cite{DBLP:conf/icse/PerezAD17} find that Shannon's entropy accurately captures the test suite's exhaustiveness:

\begin{equation}
  H(X) = - \sum_i P(x_i) \cdot log_2(P(x_i))
\end{equation}

However, an optimal entropy would require $2^M -1$ tests, which from the practical perspective is impossible to achieve.
First, not all activity patterns can be generated from tests due to ambiguity groups and software topology.
For example, a basic block consisting of several statements; these statements will always be activated together.
Second, systems of today can consist of millions of lines of code and would therefore require a non-realistic effort to generate the tests.

\subsection{Diagnosability Metric: DDU}
To elevate the problem with entropy, Perez \etal \cite{DBLP:conf/icse/PerezAD17} propose a new diagnosability metric: DDU.
DDU combines three diagnosability metrics that capture characteristics of the activity matrix, namely normalized density, diversity, and uniqueness.

\subsubsection{Density}
Prior work \cite{5954476} has used density to assess the diagnosability of the activity matrix:
\begin{equation}
  \rho = \frac{\sum_{i,j}\mathcal{A}_{ij}}{N \cdot M}
\end{equation}
Gonz\`ales-Sanchez \etal \cite{5954476} show by induction that the optimal density for SBR is obtained when $\rho = 0.5$.
For DDU, Perez \etal \cite{DBLP:conf/icse/PerezAD17} propose a normalized density $\rho'$ where its optimal value is 1.0 instead of 0.5:
\begin{equation}
  \rho' = 1 - | 1 - 2 \rho |
\end{equation}
Note that an optimal value for normalized density can be obtained without improving the diagnosability, see \Cref{subfig:no_diversity}.
For this reason, Perez \etal \cite{DBLP:conf/icse/PerezAD17} propose two enhancements: diversity and uniqueness.

\subsubsection{Diversity}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{tabular}{c|cccc}
          & $c_1$ & $c_2$ & $c_3$ & $c_4$     \\ \hline
          $t_1$ & 1 & 1 & 0 & 0 \\
          $t_2$ & 1 & 1 & 0 & 0 \\
          $t_3$ & 1 & 1 & 0 & 0 \\
          $t_4$ & 1 & 1 & 0 & 0 \\
        \end{tabular}
        \caption{No test diversity. $\rho' = 1.0$, $\mathcal{G}=0.0$}
        \label{subfig:no_diversity}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{tabular}{c|cccc}
          & $c_1$ & $c_2$ & $c_3$ & $c_4$     \\ \hline
          $t_1$ & 1 & 1 & 0 & 0 \\
          $t_2$ & 0 & 0 & 1 & 1 \\
          $t_3$ & 1 & 1 & 1 & 0 \\
          $t_4$ & 0 & 0 & 0 & 1 \\
        \end{tabular}
        \caption{Test diversity. $\rho' = 1.0$, $\mathcal{G} = 1.0$.}
        \label{subfig:diversity}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{tabular}{c|cccc}
          & $c_1$ & $c_2$ & $c_3$ & $c_4$     \\ \hline
          $t_1$ & 1 & 1 & 0 & 0 \\
          $t_2$ & 0 & 0 & 1 & 1 \\
          $t_3$ & 1 & 1 & 1 & 0 \\
          $t_4$ & 0 & 0 & 0 & 1 \\
        \end{tabular}
        \caption{Component ambiguity. $\rho' = 1.0$, $\mathcal{G}=1.0$, $\mathcal{U}=0.75$ }
        \label{subfig:ambiguity}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{tabular}{c|cccc}
          & $c_1$ & $c_2$ & $c_3$ & $c_4$     \\ \hline
          $t_1$ & 1 & 1 & 0 & 0 \\
          $t_2$ & 0 & 1 & 1 & 0 \\
          $t_3$ & 1 & 0 & 1 & 1 \\
          $t_4$ & 0 & 0 & 0 & 1 \\
        \end{tabular}
        \caption{No component ambiguity. $\rho' = 1.0$, $\mathcal{G}=1.0$, $\mathcal{U}=1.0$}
        \label{subfig:no_ambiguity}
    \end{subfigure}
    \caption{The effect of diversity and uniqueness on diagnosability.}
    \label{fig:diversity_ambiguity}
\end{figure*}

The first enhancement to the normalized density is diversity, which ensures test diversity.
Test diversity captures the diversity among test activity patterns.
The test diversity is low when the test suite consists of many tests that have identical activity patterns.
Conversely, the test diversity is high when the test suite consists of many tests that have distinct activity patterns.
From the diagnostic perspective, a high test diversity is desired such that the fault probabilities of more diagnostic candidates can be updated for a better fault diagnosis.

Perez \etal use the Gini-Simpson index $\mathcal{G}$ \cite{OIK:OIK14714} to capture test diversity:
\begin{equation}
  \mathcal{G} = 1 - \frac{\sum_{i \in 1..|G|} |g_i| \cdot (|g_i| - 1)}{N \cdot (N - 1)}
\end{equation}
where $G = \langle g_1, \dots, g_k \rangle$ is the set of ambiguity groups, and $N$ is the number of tests.
As we can observe in \Cref{subfig:diversity}, when optimizing the activity matrix for test diversity, the shortcoming of normalized density alone, shown in \Cref{subfig:no_diversity}, is mitigated.

\subsubsection{Uniqueness}
The second enhancement to the normalized density is uniqueness, which controls for the number of ambiguity groups.
An ambiguity group is a set of components that have identical activation patterns across the whole test suite, i.e. identical columns in the activity matrix.
Component ambiguity is undesirable because it prevents SBR from updating the fault probabilities of the individual components in the ambiguity group, resulting in a less accurate diagnosis.
If test suite's uniqueness is low, then many components in the activity matrix have identical activity patterns, i.e. components are involved in the same test cases.
Conversely, if the test suite's uniqueness is high, then many components in the activity matrix have distinct activity patterns.

Given the set of component ambiguity groups $H = \langle h_1, \dots, h_l \rangle$, then the test suite's uniqueness is computed as follows:
\begin{equation}
  \mathcal{U} = \frac{|H|}{M}
\end{equation}

We observe in \Cref{subfig:ambiguity} that optimizing the test suite with respect to normalized density and diversity can still result in component ambiguity groups, namely $\langle c_1, c_2 \rangle$.
When optimizing the test suite with respect to normalized density, diversity and uniqueness, we observe in \Cref{subfig:no_ambiguity} that there are no identical test activity patterns and no ambiguity groups, which results in a better diagnosability.

\subsubsection{Combined}
as follows:
\begin{equation}
 DDU = normalized\ density \cdot diversity \cdot uniqueness
\end{equation}
If $DDU = 1$, then the test suite's diagnosability is high.
Vice versa, if $DDU = 0$, then the test suite's diagnosability is low.
Perez \etal \cite{DBLP:conf/icse/PerezAD17} have shown in an experiment that optimizing a test suite with respect to DDU yields a 34\% diagnostic performance compared to a test suite optimized for branch coverage.



\chapter{Research Questions}
\label{ch:research_questions}
To explore possibilities to convey DDU to the developer with usability in mind, we define four research questions that are relevant to investigate.
Note that the nature of this study will be exploratory because little research has been performed on software diagnosability.
Moreover, DDU is a metric that has been proposed recently \cite{DBLP:conf/icse/PerezAD17} and, therefore, to the best of our knowledge, no research has investigated this metric.

\begin{framed}
\noindent
\textbf{RQ1:} What kind of values do density, diversity, uniqueness, and DDU take on?
\end{framed}

To make recommendations based on DDU, it is necessary to obtain a better understanding of DDU and its individual components: density, diversity, and uniqueness.
Specifically, we are interested in what common values are for DDU and its individual components in software systems.

\begin{framed}
\noindent
\textbf{RQ2:} What is the relation between density, diversity, uniqueness, and DDU and diagnosability?
\end{framed}

We would like to validate that DDU and diagnosability are positively correlated, i.e. the higher DDU, the better the diagnosability, and vice versa.
This question is important to answer because DDU was proposed to quantify the diagnosability.
Therefore, answering this question will function as a complementary study to Perez \etal's work \cite{DBLP:conf/icse/PerezAD17}.

\begin{framed}
\noindent
\textbf{RQ3:} What is the relation between density, diversity, uniqueness, and DDU and test coverage?
\end{framed}

The intention of test coverage is to optimize for error detection.
Perez \etal propose DDU as a complementary metric to test coverage \cite{DBLP:conf/icse/PerezAD17} because DDU is meant to capture the diagnosability and not error detection.
However, if there is a strong correlation between DDU and test coverage, then DDU could possibly replace test coverage as a test adequacy metric, which is a use case that the authors have not thought of.
Additionally, assuming that DDU is positively correlated with diagnosability and test coverage is representative for error detection, answering this question will give us a better understanding on the relation between diagnosability and error detection.


\chapter{DDU in Practice}
\label{ch:rq1}


\chapter{DDU vs. Diagnosability}
\label{ch:rq2}
In prior work Perez \etal \cite{DBLP:conf/icse/PerezAD17} show that optimizing test suite generation with respect to DDU results in better fault diagnosis.
Therefore, in this chapter, we perform experiments to verify the correlation between DDU and diagnosability, that is, answering RQ2.
We do this by discussing the experimental setup followed by the experimental results.

\begin{framed}
\noindent
\textbf{RQ2:} What is the relation between density, diversity, uniqueness, and DDU and diagnosability?
\end{framed}


\section{Experimental Setup}
\label{sec:rq2_experimental_setup}
First, we have to define the subjects of interest that will be used during the experiments.
For the experiments, we use the open source projects Commons Codec, Commons Compress, and Commons Math, which were also used in prior work by Alex \etal \cite{DBLP:conf/icse/PerezAD17}.
In addition, we include the open source projects Guice and Jsoup due to their popularity on GitHub; both roughly have 5000 stars.

To test the correlation between DDU and diagnosability, we generate 10 artificial multiple components faults of cardinality 2 for each class that has at least 8 components, i.e. method branches.
For each generated fault set, we construct an activity matrix.
We determine for each test that exercises the faulty components whether it is failing according to an \emph{oracle quality probability} of 0.75, which was also used in prior work \cite{DBLP:conf/icse/PerezAD17}.
Then, for each generated activity matrix, we use \emph{STACCATO} to generate fault candidates and \emph{BARINEL} to generate a diagnosis report.
Based on the diagnosis report we compute the wasted effort which is a measurement for diagnosability.
To account for randomness of generating fault sets, we repeat this process 10 times.
Note that we do not generate single component faults because in this case the optimal matrix for diagnosability is a diagonal activity matrix, i.e. each component each tested individually by a unit test.
Additionally, \emph{STACCATO} can sometimes take hours or days to generate fault candidates.
Hence, we discard classes when generating fault candidates takes longer than 10 seconds; this resulted in 16 classes being discarded.

\begin{figure}
  \includegraphics[width=\linewidth]{figures/fault_generation}
  \caption{An activity matrix $\mathcal{A}$ is generated for a particular class. Then, 10 fault candidates of cardinality 2 are generated with a corresponding activity matrix $\mathcal{A}_k$. For each generated matrix, we perform fault diagnosis with \emph{BARINEL} resulting in diagnostic report $\mathcal{D}_k$ and compute the wasted effort. Finally, we compute the average wasted effort. This process is repeated 10 times.}
  \label{fig:fault_generation}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{figures/fault_generation_delta}
  \caption{Two activity matrices $\mathcal{A}$ and $\mathcal{A}'$ are generated for a particular class based on two different test suites. We generate 10 fault candidates of cardinality 2 and accordingly generate 10 activity matrices. Then, we use \emph{BARINEL} to perform fault diagnosis and compute the wasted effort.}
  \label{fig:fault_generation_delta}
\end{figure}

In the construction of the activity matrix we use the branch granularity, that is, every component of the activity matrix represents a method branch; this granularity is also used by Perez \etal \cite{DBLP:conf/icse/PerezAD17}.
To construct the activity matrix of a class we use Perez' DDU Maven plugin\footnote{\url{https://github.com/aperez/ddu-maven-plugin}} using the \texttt{basicblock} granularity, which represents branch granularity.
The steps after the obtaining the activity matrix in \Cref{fig:fault_generation} are performed using Python scripts\footnote{\url{https://github.com/aaronang/ddu}}.

This experiment is different from Perez \etal's work because we do not improve the DDU of a fixed system.
Specifically, in Perez \etal's study, the authors improved the DDU of a fixed system under test by generating new test cases using \emph{EvoSuite}.
However, in this experiment, we compute the DDU for each class and measure for each class its diagnosability using the aforementioned approach.
Essentially, the difference is that we do not improve the DDU of a fixed system but we are simply measuring the DDU.



For this reason, we perform another experiment where we generate two test suites for each class with at least 8 components and at least 10 unit tests.
In addition, we perform the same experiment but for all classes with at least 8 components.
We generate two test suites by using all test cases and 50\% of the test cases.
For both test suites we compute the DDU and randomly generate 10 multiple components faults of cardinality 2 to compute the wasted effort.
Similar to previous experiment we perform this process 10 times to account for randomness of generating fault sets.
The intuition behind this experiment is when we improve the DDU of a fixed system, its diagnosability should improve too.
The setup of this experiment is illustrated in \Cref{fig:fault_generation_delta}.

\section{Experimental Results}
\label{sec:rq2_experimental_results}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ddu_effort_goodness_025}
        \caption{DDU, $r = -0.046$, $p = 0.224$.}
        \label{fig:ddu_effort_goodness_025}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/density_effort_goodness_025}
        \caption{Density, $r = 0.183$, $p < 0.001$.}
        \label{fig:density_effort_goodness_025}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/diversity_effort_goodness_025}
        \caption{Diversity, $r = -0.326$, $p < 0.001$.}
        \label{fig:diversity_effort_goodness_025}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/uniqueness_effort_goodness_025}
        \caption{Uniqueness, $r = -0.120$, $p < 0.001$.}
        \label{fig:uniqueness_effort_goodness_025}
    \end{subfigure}
    \caption{Scatterplot of density, diversity, uniqueness, and DDU against effort.}
    \label{fig:metric_vs_effort}
\end{figure*}

In the first experiment, we measure for each class the density, diversity, uniqueness, DDU, and diagnosability.
The results of this experiment are shown in \Cref{fig:metric_vs_effort}.
Note that in \Cref{fig:metric_vs_effort} the population comprises all classes of all projects.
Each datapoint in \Cref{fig:metric_vs_effort} represents a class for which 100 fault candidates are generated in (potentially overlapping) sets of 10 fault candidates as described in \Cref{fig:fault_generation}.
We observe the that there is a positive correlation between density and effort, a negative correlation between diversity and effort, a negative correlation between uniqueness and effort, and a statistically non-significant weak correlation between DDU and effort.

To investigate the relations between these metrics in more detail, we display the correlation values per project in \Cref{tab:correlation_effort}.
For three projects we can say with 95\% confidence that density is correlated with effort.
However, the Pearson correlation for Commons Compress is negative while the Pearson correlation values for Commons Math and Commons Codec are positive.
Hence, the results show no strong evidence that density is correlated with effort.
Regarding diversity and uniqueness, we observe in \Cref{tab:correlation_effort} and \Cref{fig:metric_vs_effort} that both metrics have a weak negative correlation with effort, and that the correlation values in 4 out of 5 projects are statistically significant.
Regarding DDU, for two projects the results show statistical significance that DDU is negatively correlated to effort.
However, for three projects there is no evidence that DDU is correlated to effort.
Therefore, there is no strong evidence that DDU is negatively correlated to effort.

\begin{table}[]
\centering
\caption{Correlation between density, diversity, uniqueness, DDU, and effort.}
\label{tab:correlation_effort}
\begin{adjustbox}{center}
\begin{tabular}{llllll}
\toprule
 &  & \multicolumn{4}{c}{Pearson correlation / Correlation p-value} \\ \cline{3-6}
\multirow{-2}{*}{Subject} & \multirow{-2}{*}{Size} & Density & Diversity & Uniqueness & DDU \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.63 & -0.33 & -0.65 & -0.23 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Codec\end{tabular}} & \multirow{-2}{*}{\cellcolor{Gray}34} & $\mathbf{5.880\times10^{-5}}$ & 0.057 & $\mathbf{3.713\times10^{-5}}$ & 0.197 \\
 &  & -0.22 & -0.45 & -0.40 & -0.37 \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Commons\\ Compress\end{tabular}} & \multirow{-2}{*}{104} & $\mathbf{0.027}$ & $\mathbf{1.348\times10^{-6}}$ & $\mathbf{2.083\times10^{-5}}$ & $\mathbf{1.121\times10^{-4}}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.20 & -0.36 & -0.19 & -0.03 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Math\end{tabular}} & \multirow{-2}{*}{\cellcolor{Gray}420} & $\mathbf{4.982\times10^{-5}}$ & $\mathbf{1.782\times10^{-14}}$ & $\mathbf{7.553\times10^{-5}}$ & 0.572 \\
 &  & 0.01 & -0.31 & -0.29 & -0.22 \\
\multirow{-2}{*}{Guice} & \multirow{-2}{*}{94} & 0.935 & $\mathbf{0.002}$ & $\mathbf{0.005}$ & $\mathbf{0.031}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.29 & -0.37 & 0.16 & 0.20 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}Jsoup} & \multirow{-2}{*}{\cellcolor{Gray}37} & 0.085 & $\mathbf{0.024}$ & 0.337 & 0.229\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/delta_density_delta_effort}
        \caption{Density, $r=-0.107$, $p<0.01$.}
        \label{fig:delta_density_delta_effort}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/delta_diversity_delta_effort}
        \caption{Diversity, $r=-0.189$, $p = 4.479\times10^{-5}$.}
        \label{fig:delta_diversity_delta_effort}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/delta_uniqueness_delta_effort}
        \caption{Uniqueness, $r=-0.079$, $p=0.137$.}
        \label{fig:delta_uniqueness_delta_effort}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/delta_ddu_delta_effort}
        \caption{DDU, $r=-0.223$, $p=9.044\times10^{-7}$.}
        \label{fig:delta_ddu_delta_effort}
    \end{subfigure}
    \caption{Scatterplot of delta density, delta diversity, delta uniqueness, and delta DDU against delta effort.}
    \label{fig:delta_metric_vs_delta_effort}
\end{figure*}

\begin{table}[]
\centering
\caption{Correlation between delta density, delta diversity, delta uniqueness, delta DDU, and delta effort.}
\label{tab:delta_metric_delta_effort}
\begin{adjustbox}{center}
\begin{tabular}{lllll}
\toprule
 & \multicolumn{4}{c}{Size / Pearson correlation / Correlation p-value} \\
\multirow{-2}{*}{Subject} & Density & Diversity & Uniqueness & DDU \\
\rowcolor{Gray}
\cellcolor{Gray} & 29 & 24 & 26 & 29 \\
\rowcolor{Gray}
\cellcolor{Gray} & -0.277 & -0.229 & 0.230 & 0.011 \\
\rowcolor{Gray}
\multirow{-3}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Codec\end{tabular}} & 0.145 & 0.281 & 0.256 & 0.950 \\
 & 74 & 73 & 55 & 74 \\
 & -0.098 & -0.372 & -0.217 & -0.297 \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}l@{}}Commons\\ Compress\end{tabular}} & 0.401 & $\mathbf{0.001}$ & 0.111 & $\mathbf{0.010}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & 251 & 256 & 186 & 262 \\
\rowcolor{Gray}
\cellcolor{Gray} & -0.142 & -0.162 & -0.109 & -0.280 \\
\rowcolor{Gray}
\multirow{-3}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Math\end{tabular}} & $\mathbf{0.023}$ & $\mathbf{0.009}$ & 0.135 & $\mathbf{3.855\times10^{-6}}$ \\
 & 78 & 78 & 57 & 78 \\
 & 0.103 & 0.001 & 0.047 & -0.006 \\
\multirow{-3}{*}{Guice} & 0.368 & 0.995 & 0.728 & 0.952 \\
\rowcolor{Gray}
\cellcolor{Gray} & 29 & 29 & 27 & 29 \\
\rowcolor{Gray}
\cellcolor{Gray} & 0.452 & -0.465 & 0.012 & -0.273 \\
\rowcolor{Gray}
\multirow{-3}{*}{\cellcolor{Gray}Jsoup} & $\mathbf{0.013}$ & $\mathbf{0.011}$ & 0.951 & 0.150\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

In the second experiment, we generate two test suites for a given class: a test suite with 50\% of the test cases enabled, and a test suite with 100\% of the test cases enabled.
This naturally results in two test suites with two different DDU values.
For each class, we compare the effort of a test suite with a lower DDU value with a test suite with a higher DDU value.
Identical approach is used for density, diversity, and uniqueness.
Note that we exclude classes where the two test suites do not result in a metric difference.
The results of this experiment are shown in \Cref{fig:delta_metric_vs_delta_effort} and \Cref{tab:delta_metric_delta_effort}.
In \Cref{fig:delta_metric_vs_delta_effort}, we observe that an increase in any metric --- density, diversity, uniqueness, DDU --- results in a lower required effort to diagnose mistakes.
Although the results are statistically significant, see \Cref{fig:delta_metric_vs_delta_effort}, the correlations are weak.

Revisiting the second research question:
\begin{framed}
\noindent
\textbf{RQ2:} What is the relation between density, diversity, uniqueness, and DDU and diagnosability?
\end{framed}
\textbf{A:} In the first experiment, two out of the five projects show that DDU is negatively correlated with diagnosability.
In the second experiment, two out of the five projects show that improving the DDU value of a class' test suite will improve the diagnosability.
Therefore, based on these two experiments, there is no evidence that DDU is strongly correlated to diagnosability.
However, in the second experiment, we observe that an increase in DDU is weakly correlated to an decrease in effort and, thus, weakly correlated to diagnosability.
There is also no strong evidence that density, diversity, and uniqueness are correlated to diagnosability.

\chapter{DDU vs. Error Detection}
\label{ch:rq3}
In this chapter, we discuss

\begin{table}[]
\centering
\caption{Correlation between density, diversity, uniqueness, DDU, and branch coverage.}
\label{tab:correlation_branch_coverage}
\begin{adjustbox}{center}
\begin{tabular}{llllll}
\toprule
 &  & \multicolumn{4}{c}{Pearson correlation / Correlation p-value} \\
\multirow{-2}{*}{Subject} & \multirow{-2}{*}{Size} & Density & Diversity & Uniqueness & DDU \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.01 & 0.66 & 0.66 & 0.54 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Codec\end{tabular}} & \multirow{-2}{*}{\cellcolor{Gray}35} & 0.909 & $\mathbf{1.27\times10^{-5}}$ & $\mathbf{0.001}$ & $\mathbf{7.095\times10^{-4}}$ \\
 &  & 0.21 & 0.42 & 0.38 & 0.37 \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Commons\\ Compress\end{tabular}} & \multirow{-2}{*}{108} & $\mathbf{0.024}$ & $\mathbf{3.627\times10^{-6}}$ & $\mathbf{4.736\times10^{-5}}$ & $\mathbf{5.142\times10^{-5}}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & -0.04 & 0.18 & 0.16 & 0.16 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Math\end{tabular}} & \multirow{-2}{*}{\cellcolor{Gray}433} & 0.307 & $\mathbf{9.635\times10^{-5}}$ & $\mathbf{7.426\times10^{-4}}$ & $\mathbf{3.848\times10^{-4}}$ \\
 &  & 0.04 & 0.07 & 0.34 & 0.25 \\
\multirow{-2}{*}{Guice} & \multirow{-2}{*}{96} & 0.672 & 0.445 & $\mathbf{4.969\times10^{-4}}$ & $\mathbf{0.011}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.36 & -0.31 & 0.23 & 0.29 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}Jsoup} & \multirow{-2}{*}{\cellcolor{Gray}38} & $\mathbf{0.023}$ & 0.057 & 0.163 & 0.070\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\centering
\caption{Correlation between density, diversity, uniqueness, DDU, and error detection.}
\label{tab:correlation_error_detection}
\begin{adjustbox}{center}
\begin{tabular}{llllll}
\toprule
 &  & \multicolumn{4}{c}{Pearson correlation / Correlation p-value} \\
\multirow{-2}{*}{Subject} & \multirow{-2}{*}{Size} & Density & Diversity & Uniqueness & DDU \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.64 & 0.15 & -0.17 & 0.24 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Codec\end{tabular}} & \multirow{-2}{*}{\cellcolor{Gray}35} & $\mathbf{2.973\times10^{-5}}$ & 0.366 & 0.313 & 0.163 \\
 &  & 0.25 & 0.18 & 0.15 & 0.27 \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Commons\\ Compress\end{tabular}} & \multirow{-2}{*}{108} & $\mathbf{0.008}$ & 0.052 & 0.109 & $\mathbf{0.004}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.17 & -0.18 & -0.145 & 0.06 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}\begin{tabular}[c]{@{}l@{}}Commons\\ Math\end{tabular}} & \multirow{-2}{*}{\cellcolor{Gray}433} & $\mathbf{1.835\times10^{-4}}$ & $\mathbf{1.281\times10^{-4}}$ & $\mathbf{0.002}$ & 0.146 \\
 &  & 0.28 & 0.020 & 0.12 & 0.27 \\
\multirow{-2}{*}{Guice} & \multirow{-2}{*}{96} & $\mathbf{0.004}$ & 0.842 & 0.207 & $\mathbf{0.006}$ \\
\rowcolor{Gray}
\cellcolor{Gray} & \cellcolor{Gray} & 0.59 & -0.26 & -0.09 & 0.40 \\
\rowcolor{Gray}
\multirow{-2}{*}{\cellcolor{Gray}Jsoup} & \multirow{-2}{*}{\cellcolor{Gray}38} & $\mathbf{8.175\times10^{-5}}$ & 0.104 & 0.560 & $\mathbf{0.010}$\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\chapter{Conclusion}
\label{ch:conclusion}


\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
